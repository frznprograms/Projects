# -*- coding: utf-8 -*-
"""reddit_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qibx9seJidVP9jgwtpyhofoyyMnh-Gl2

## Imports ##
"""

import nltk
from nltk.corpus import words
import pandas as pd
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

"""## Data Preparation ##"""

nltk.download("words")

data = pd.read_csv("kaggle_RC_2019-05.csv")
data.head()

# remove controversial statements
data = data[data["controversiality"] != 1]
#data = data[["body"]]
print(data.head())

corpus = set(words.words())

def clean_text(string):
  list_ = string.split(" ")
  # convert all to lower case except special case: "I"
  lower_cases = [word.lower() for word in list_ if word != "I"]
  # only include words that appear in the english language
  valid_words = [word for word in lower_cases if word in corpus]
  return valid_words

data['body'] = data['body'].apply(lambda x: clean_text(x))
print(data.head())

all_dataset_words = []
def compile_all_words(word_list):
  all_dataset_words.extend(word_list)
  return word_list

data['body'] = data['body'].apply(compile_all_words)

all_unique_words = set(all_dataset_words)

num_unique_words = len(all_unique_words)
num_unique_words

# create mapping of words to their numbers and vice versa
string_to_int = {s: i for i, s in enumerate(all_unique_words)}
int_to_string = {i: s for s, i in string_to_int.items()}

# build training data and labels

block_size = 3  # trigram model
X, y = [], []

for index, row in data.iterrows():
  word_list = row['body']
  for i in range(len(word_list) - block_size + 1):
    context = word_list[i:i + block_size - 1]  # context (2 words for trigram)
    target = word_list[i + block_size - 1]  # target (3rd word in trigram)

    # Convert context and target to integer representation
    context_indices = [string_to_int[word] for word in context]
    target_index = string_to_int[target]

    X.append(context_indices)
    y.append(target_index)

X = torch.tensor(X)
y = torch.tensor(y)

X.shape, y.shape

# create embedding tensor -> 90 unique words will be embedded each into 10 values
C = torch.randn((num_unique_words, 10))

C.shape

# lookup of embeddings for every context tensor in X
emb = C[X]
emb.shape

"""## Split into Training and Testing ##"""

# n_samples = len(X)
# reduced_size = n_samples // 10000  # Reduce data size
# indices = torch.randperm(n_samples)[:reduced_size]
# X_reduced = X[indices]
# y_reduced = y[indices]


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=1
)

print(f"Training data: X_train={X_train.shape}, y_train={y_train.shape}")
print(f"Testing data: X_test={X_val.shape}, y_test={y_val.shape}")
print(f"Testing data: X_test={X_test.shape}, y_test={y_test.shape}")

"""## Prepare Device, Move Data to Device ##"""

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Currently using {device}")

X_train = X_train.to(device)
X_val = X_val.to(device)
X_test = X_test.to(device)
y_train = y_train.to(device)
y_val = y_val.to(device)
y_test = y_test.to(device)

X_train.shape, X_val.shape, X_test.shape

y_train.shape, y_val.shape, y_test.shape

"""## Preparing Torch Dataset and DataLoader ##"""

# create custom dataset type
class TextPredictionDataset(Dataset):
  def __init__(self, X, y):
    self.X = X
    self.y = y

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return self.X[idx], self.y[idx]

BATCH_SIZE = 64
train_dataset = TextPredictionDataset(X_train, y_train)
val_dataset = TextPredictionDataset(X_val, y_val)
test_dataset = TextPredictionDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""## Create Trigram Model ##"""

class TrigramModel(nn.Module):
  def __init__(self, vocab_size, embedding_dim, block_size, hidden_size):
    super().__init__()
    self.C = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer
    self.linear_stack = nn.Sequential(
      nn.Linear(block_size * embedding_dim, hidden_size),
      nn.Linear(hidden_size, hidden_size),
      nn.Linear(hidden_size, hidden_size),
      nn.Tanh(),
      nn.Linear(hidden_size, vocab_size)  # Output layer
    )

  def forward(self, X):
    emb = self.C(X)  # Convert indices to embeddings
    emb = emb.view(emb.shape[0], -1)
    logits = self.linear_stack(emb)  # Pass through the linear stack
    return logits

model = TrigramModel(num_unique_words, 10, 2, 300)
model

"""## Model Training ##"""

torch.manual_seed(1)

# Hyperparameters
EMBEDDING_DIM = 10
BLOCK_SIZE = 2  # Trigram context size
HIDDEN_SIZE = 100
VOCAB_SIZE = num_unique_words
EPOCHS = 100
LR = 0.01

def train_model(embedding_dim, block_size, hidden_size, vocab_size, epochs=1000, lr=0.01):
  # Initialize model, loss function, and optimizer
  model = TrigramModel(vocab_size, embedding_dim, block_size, hidden_size)
  model.to(device)
  loss_fun = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)

  for epoch in range(epochs):
    # training
    model.train()
    total_train_loss = 0
    for batch_X, batch_y in train_loader:
      batch_X, batch_y = batch_X.to(device), batch_y.to(device)
      y_pred = model(batch_X)
      loss = loss_fun(y_pred, batch_y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      total_train_loss = loss.item()
      # preserve gpu memory
      del batch_X, batch_y
      torch.cuda.empty_cache()

    # validation
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
      for batch_X, batch_y in val_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        val_pred = model(batch_X)
        val_loss = loss_fun(val_pred, batch_y)
        total_val_loss += val_loss.item()
        del batch_X, batch_y
        torch.cuda.empty_cache()

    # log metrics
    if epoch % 10 == 0:
      print(
          f"Epoch: {epoch} | Training Loss: {total_train_loss / len(train_loader):.4f} "
          f"| Validation Loss: {total_val_loss / len(val_loader):.4f}"
      )

  return model

trained_model = train_model(
    embedding_dim=EMBEDDING_DIM,
    block_size=BLOCK_SIZE,
    hidden_size=HIDDEN_SIZE,
    vocab_size=VOCAB_SIZE,
    epochs=EPOCHS,
    lr=LR
)

def evaluate_model(model, dataloader, criterion, device):
  model.eval()
  total_loss = 0
  all_preds = []
  all_labels = []

  with torch.no_grad():
    for batch in dataloader:
      # unpack batch of data
      X_batch, y_batch = batch
      X_batch, y_batch = X_batch.to(device), y_batch.to(device)

      logits = model(X_batch)
      loss = criterion(logits, y_batch)
      total_loss += loss.item()

      preds = torch.argmax(logits, dim=1)
      all_preds.extend(preds.cpu().numpy())
      all_labels.extend(y_batch.cpu().numpy())

      del X_batch, y_batch
      torch.cuda.empty_cache()

  accuracy = accuracy_score(all_labels, all_preds)
  avg_loss = total_loss / len(dataloader)

  print(f"Test Loss: {avg_loss:.4f}")
  print(f"Test Accuracy: {accuracy:.4f}")

  return avg_loss, accuracy

test_loss, test_accuracy = evaluate_model(
    model, test_loader, nn.CrossEntropyLoss(), device
)

print(f"Test loss: {test_loss}, Test Accuracy: {test_accuracy}")
