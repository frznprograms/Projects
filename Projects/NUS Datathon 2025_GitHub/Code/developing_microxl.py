# -*- coding: utf-8 -*-
"""developing_microxl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rGMO8pAOv2yIoclRZgPBu5OXFGdQdm8t

# Imports #
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall -y tensorflow keras tensorflow-recommenders tensorflow-text tensorflow-tpu tf-keras
# %pip install -U tensorflow==2.17 keras==3.5.0 tensorflow-recommenders tensorflow-text

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q tensorflow-recommenders
# %pip install --upgrade tensorflow_ranking

import pandas as pd
import pyarrow.parquet as pq
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
from sklearn.preprocessing import LabelEncoder
from datetime import datetime
import tensorflow as tf
from scipy.sparse import coo_matrix
from typing import Dict, Text
from sklearn.utils import resample

import tensorflow_recommenders as tfrs
import tensorflow_ranking as tfr

#drive.mount("/content/drive/")

"""# Data Preparation #

## Data Cleaning ##

### Preparing var_1 Data ###
"""

var_1_df = pd.read_parquet("")

var_1_df.columns

# drop columns to maintain ethical standards
var_1_df.drop(columns=[
    'race_desc_map',
    'cltpcode',
    'household_size',
    'family_size'
], axis=1, inplace=True)

cols_to_encode = ['marryd', 'cltsex', 'household_size_grp', 'family_size_grp']
for col in cols_to_encode:
    encoder = LabelEncoder()
    var_1_df[col] = encoder.fit_transform(var_1_df[col])

this_year = datetime.now().year
var_1_df['age'] = this_year - var_1_df['cltdob'].dt.year

var_1_df.drop(columns=[
    'cltdob'
], axis=1, inplace=True)

var_1_df['economic_status'] = pd.to_numeric(var_1_df['economic_status'], errors='coerce')
var_1_df['economic_status'] = var_1_df['economic_status'].fillna(var_1_df['economic_status'].median())
var_1_df['economic_status'] = var_1_df['economic_status'].astype('int32')

var_1_df['age'] = var_1_df['age'].fillna(var_1_df['age'].median()).astype('int32')

var_1_df['cltsex'] = var_1_df['cltsex'].astype('int32')
var_1_df['household_size_grp'] = var_1_df['household_size_grp'].astype('int32')
var_1_df['family_size_grp'] = var_1_df['family_size_grp'].astype('int32')
var_1_df['marryd'] = var_1_df['marryd'].astype('int32')

var_1_df['secuityno'] = var_1_df['secuityno'].apply(lambda string: string[4:])
var_1_df['secuityno'] = var_1_df['secuityno'].astype('int32')

"""### Preparing var_2 Data ###"""

var_2_df = pd.read_parquet("")

test = var_2_df[var_2_df['product'] == 'prod_6']
print(test['annual_premium'].mean())

# Maximum count (12082 for product 8.0)
max_count = var_2_df['product'].value_counts().max()

# Initialize an empty list to hold the resampled data
oversampled_df = []

# Loop over each unique product and resample
for product, count in var_2_df['product'].value_counts().items():
    product_df = var_2_df[var_2_df['product'] == product]
    if count < max_count:
        # Resample the product to match the max_count
        product_df_oversampled = resample(
            product_df, replace=True, n_samples=max_count - count, random_state=42
        )
        oversampled_df.append(product_df_oversampled)

    oversampled_df.append(product_df)

# Concatenate the list of resampled dataframes back into one dataframe
oversampled_var_2_df = pd.concat(oversampled_df)

var_2_df = oversampled_var_2_df

var_2_df['cust_tenure_at_purchase_grp'].value_counts()
encoder = LabelEncoder()
var_2_df['cust_tenure_at_purchase_grp'] = encoder.fit_transform(var_2_df['cust_tenure_at_purchase_grp'])

var_2_df['product_grp'].value_counts()
encoder = LabelEncoder()
var_2_df['product_grp'] = encoder.fit_transform(var_2_df['product_grp'])

len(var_2_df)

var_2_df.drop(columns=[
    'flg_main', 'flg_rider', 'flg_inforce',
    'flg_lapsed', 'flg_cancel', 'flg_expire',
    'flg_converted', 'occdate', 'chdrnum',
    'agntnum', 'cust_age_at_purchase_grp'
], axis=1, inplace=True)

var_2_df['product'] = var_2_df['product'].apply(lambda string: string[-1:])
var_2_df['product'] = var_2_df['product'].astype('int32')

prod_types = list(var_2_df['product'].unique())
mean_values = {}
for prod in prod_types:
  subset_data = var_2_df[var_2_df['product'] == prod]
  mean_value = subset_data['annual_premium'].mean()
  mean_values[prod] = mean_value

var_2_df['mean_premium'] = var_2_df['product'].map(mean_values)

var_2_df.drop(columns=['annual_premium'], axis=1, inplace=True)

var_2_df['secuityno'] = var_2_df['secuityno'].apply(lambda string: string[4:])
var_2_df['secuityno'] = var_2_df['secuityno'].astype('int32')

var_2_df['product'].unique()

"""### Preparing var_3 Data ###"""

var_3_df = pd.read_parquet("")

var_3_df.columns

var_3_df.drop(columns=[
    'var_3_age', 'var_3_gender', 'var_3_marital', 'var_3_tenure',
    'cnt_converted', 'annual_premium_cnvrt', 'pct_lapsed', 'pct_cancel',
    'pct_inforce', 'pct_SX0_unknown',
    'pct_SX1_male', 'pct_SX2_female', 'pct_AG01_lt20', 'pct_AG02_20to24',
    'pct_AG03_25to29', 'pct_AG04_30to34', 'pct_AG05_35to39',
    'pct_AG06_40to44', 'pct_AG07_45to49', 'pct_AG08_50to54',
    'pct_AG09_55to59', 'pct_AG10_60up',
], axis=1, inplace=True)

type(var_3_df['var_3_product_expertise'].iloc[0][0])

def process_expertise(item):
    new_list = []
    for elem in item:
        new_list.append(int(elem[-1]))
    return new_list

var_3_df['var_3_product_expertise'] = var_3_df['var_3_product_expertise'].apply(lambda item: process_expertise(item))

"""### Summarizing the Data ###"""

var_2_df.columns

var_1_not_in_var_2 = var_1_df[~var_1_df['secuityno'].isin(var_2_df['secuityno'])]
print(len(var_1_not_in_var_2['secuityno']))

merged_data = pd.merge(var_1_df, var_2_df, on='secuityno', how='outer')

merged_data.head()

var_1_df = merged_data[['secuityno', 'marryd', 'cltsex', 'economic_status', 'household_size_grp',
       'family_size_grp', 'age', 'cust_tenure_at_purchase_grp']]

var_2_df = merged_data[['secuityno', 'product', 'product_grp', 'mean_premium']]

"""# Final Tensors and Inputs #

The matrix created in this step will serve as A, which can then be used for matrix factorization where A = UV^T.
"""

var_2_df["mean_premium"] = (var_2_df["mean_premium"] - var_2_df["mean_premium"].mean()) / var_2_df["mean_premium"].std()
var_1_df["age"] = (var_1_df["age"] - var_1_df["age"].mean()) / var_1_df["age"].std()

unique_var_1s = var_1_df["secuityno"].unique()
unique_var_3s = var_2_df["product"].unique()

# Create mappings for var_1 and var_2 IDs to matrix indices
var_1_to_index = {var_1: idx for idx, var_1 in enumerate(unique_var_1s)}
var_2_to_index = {var_2: idx for idx, var_2 in enumerate(unique_var_3s)}
index_to_var_2 = {idx: var_2 for var_2, idx in var_2_to_index.items()}  # Reverse mapping

# Number of var_1s and var_3s
num_var_1s = len(unique_var_1s)
num_var_3s = len(unique_var_3s)

# Manually set correct feature sizes
var_1_feature_size = len(["marryd", "cltsex", "economic_status", "household_size_grp", "family_size_grp", "age", "cust_tenure_at_purchase_grp"])
var_2_feature_size = len(["product_grp", "mean_premium"])

data = []
for var_1 in unique_var_1s:
    var_1_id = var_1_to_index[var_1]
    var_3s_var_1_bought = var_2_df[var_2_df['secuityno'] == var_1]['product'].unique()
    var_3s_var_1_didnt_buy = [p for p in unique_var_3s if p not in var_3s_var_1_bought]

    var_1_features = var_1_df.loc[var_1_df['secuityno'] == var_1, [
        "marryd", "cltsex", "economic_status", "household_size_grp", "family_size_grp", "age", "cust_tenure_at_purchase_grp"
    ]].values.flatten().astype(np.float32)

    # Ensure fixed shape (truncate if too long, pad if too short)
    var_1_features = np.resize(var_1_features, var_1_feature_size)

    for var_2 in var_3s_var_1_bought:
        var_2_id = var_2_to_index[var_2]
        var_2_features = var_2_df[var_2_df['product'] == var_2][["product_grp", "mean_premium"]].values.flatten().astype(np.float32)
        var_2_features = np.resize(var_2_features, var_2_feature_size)
        data.append((var_1_id, var_2_id, var_1_features, var_2_features, 1))

    for var_2 in var_3s_var_1_didnt_buy:
        var_2_id = var_2_to_index[var_2]
        var_2_features = var_2_df[var_2_df['product'] == var_2][["product_grp", "mean_premium"]].values.flatten().astype(np.float32)
        var_2_features = np.resize(var_2_features, var_2_feature_size)
        data.append((var_1_id, var_2_id, var_1_features, var_2_features, 0))



# Convert data to TensorFlow dataset
def convert_to_tf_dataset(data):
    var_1_ids, var_2_ids, var_1_features, var_2_features, labels = zip(*data)

    # Ensure features are stacked properly to form rectangular arrays
    var_1_features = np.stack(var_1_features).astype(np.float32)
    var_2_features = np.stack(var_2_features).astype(np.float32)
    labels = np.array(labels, dtype=np.float32)

    # Ensure labels are exactly 0 or 1
    print("Unique labels:", np.unique(labels))

    dataset = tf.data.Dataset.from_tensor_slices({
        "var_1_id": np.array(var_1_ids, dtype=np.int32),
        "var_2_id": np.array(var_2_ids, dtype=np.int32),
        "var_1_features": var_1_features,
        "var_2_features": var_2_features,
        "label": labels
    })
    return dataset

interaction_dataset = convert_to_tf_dataset(data)





"""# Model Development #

## Model Training ##
"""

np.random.seed(40)
tf.random.set_seed(40)

# Shuffle and split dataset
dataset_size = interaction_dataset.cardinality().numpy()  # Get the dataset size
train_size = int(0.8 * dataset_size)

shuffled = interaction_dataset.shuffle(dataset_size, seed=40, reshuffle_each_iteration=False)
train = shuffled.take(train_size)
test = shuffled.skip(train_size)

class var_1var_2Modeller(tf.keras.Model):
    def __init__(self, num_var_1s, num_var_3s, embedding_dim=64):
        super().__init__()

        # Reduce var_1 ID embedding size
        self.var_1_embeddings = tf.keras.layers.Embedding(num_var_1s, embedding_dim // 2)
        self.var_2_embeddings = tf.keras.layers.Embedding(num_var_3s, embedding_dim // 2)

        # Increase metadata feature embeddings
        self.var_1_dense = tf.keras.layers.Dense(embedding_dim, activation='relu')
        self.var_1_dropout = tf.keras.layers.Dropout(0.4)
        self.var_2_dense = tf.keras.layers.Dense(embedding_dim, activation='relu')
        self.var_2_dropout = tf.keras.layers.Dropout(0.4)

        # Final projection layer to align dimensions
        self.final_dense = tf.keras.layers.Dense(embedding_dim, activation='relu')

    def call(self, inputs: Dict[Text, tf.Tensor]):
        var_1 = inputs["var_1_id"]
        var_2 = inputs["var_2_id"]
        var_1_features = inputs["var_1_features"]
        var_2_features = inputs["var_2_features"]

        # Embed var_1 ID and var_2 ID
        var_1_emb = self.var_1_embeddings(var_1)
        var_2_emb = self.var_2_embeddings(var_2)

        # Process metadata features
        var_1_feat = self.var_1_dense(var_1_features)
        var_2_feat = self.var_2_dense(var_2_features)

        # Concatenate embeddings and metadata features
        combined_var_1 = tf.concat([var_1_emb, var_1_feat], axis=1)
        combined_var_2 = tf.concat([var_2_emb, var_2_feat], axis=1)

        # Final projection to align dimensions
        combined_var_1 = self.final_dense(combined_var_1)
        combined_var_2 = self.final_dense(combined_var_2)

        return tf.keras.activations.sigmoid(tf.reduce_sum(combined_var_1 * combined_var_2, axis=1, keepdims=True))

@tf.keras.saving.register_keras_serializable()
class MicroXL(tfrs.models.Model):
    def __init__(self, num_var_1s, num_var_3s):
        super().__init__()
        self.ranking_model = var_1var_2Modeller(num_var_1s, num_var_3s)
        self.task = tfrs.tasks.Ranking(
            loss=tf.keras.losses.BinaryCrossentropy(),
            #metrics=[tfr.keras.metrics.NDCGMetric(name="ndcg")]
        )

    def build(self, input_shape):
        self.ranking_model.build(input_shape)

    def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:
        return self.ranking_model(features)

    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:
        labels = features["label"]
        rating_predictions = self(features)
        return self.task(labels=labels, predictions=rating_predictions)

# Detect TPU and initialize
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)  # Use TPU strategy
    print("Running on TPU:", tpu.master())
except ValueError:
    strategy = tf.distribute.get_strategy()  # Default to CPU/GPU
    print("Running on CPU/GPU")

NUM_var_1S = var_1_df['secuityno'].nunique()
NUM_var_3s = var_2_df['product'].nunique()

NUM_var_1S, NUM_var_3s

np.random.seed(40)
tf.random.set_seed(40)

with strategy.scope():  # Place model inside TPU scope
    model = MicroXL(num_var_1s=NUM_var_1S, num_var_3s=NUM_var_3s)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003))

# Train the model
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
model.fit(train.batch(128), epochs=10  , callbacks=[callback])

model.save('XLMicro_v3.keras')

"""## Model Testing/Inference ##"""

# Function to recommend var_3s
def recommend_var_3s(model, var_1_id, var_1_features, top_k=2):
    var_2_ids = np.array(list(var_2_to_index.values()), dtype=np.int32)

    # Ensure all unique var_3s are retrieved
    var_2_features = var_2_df.groupby("product").first().reindex(unique_var_3s)[["product_grp", "mean_premium"]]
    var_2_features = var_2_features.fillna(0).to_numpy(dtype=np.float32)  # Handle any missing values

    var_1_ids = np.full((num_var_3s,), var_1_id, dtype=np.int32)
    var_1_features = np.tile(var_1_features.reshape(1, -1), (num_var_3s, 1))

    # Ensure correct shape for var_2_features
    if var_2_features.shape[0] != num_var_3s:
        raise ValueError(f"var_2_features shape mismatch: Expected ({num_var_3s}, {var_2_feature_size}), but got {var_2_features.shape}")

    inputs = {
        "var_1_id": var_1_ids,
        "var_2_id": var_2_ids,
        "var_1_features": var_1_features.astype(np.float32),
        "var_2_features": var_2_features.astype(np.float32)  # Ensure proper shape
    }

    scores = model(inputs).numpy().flatten()
    top_k_indices = np.argsort(scores)[-top_k:][::-1]
    recommended_var_2_ids = [index_to_var_2[idx] for idx in top_k_indices]  # Convert back to original var_2 IDs
    return recommended_var_2_ids, scores[top_k_indices]



"""### Model Evaluation ###"""

# Function to evaluate model
def evaluate_model(model, test_data, top_k=2):
    correct_predictions = 0
    total_samples = 0

    for test_sample in test_data:
        var_1_id = test_sample["var_1_id"].numpy()
        true_var_2 = test_sample["var_2_id"].numpy()
        var_1_features = test_sample["var_1_features"].numpy()

        # Convert true_var_2 to original if needed
        if true_var_2 in index_to_var_2:
            true_var_2 = index_to_var_2[true_var_2]  # Ensure consistency with mapped var_3s

        recommended_var_3s, _ = recommend_var_3s(model, var_1_id, var_1_features, top_k)

        if true_var_2 in recommended_var_3s:
            correct_predictions += 1

        total_samples += 1

    accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0
    print(f"Model Top-{top_k} Accuracy: {accuracy:.4f}")
    return accuracy

model = tf.keras.models.load_model('XLMicro_v3.keras')

"""How well does the model generalize for new, unseen users?"""

evaluate_model(model, test, top_k=5)

"""How well does the model generalize for users it already knows?"""

evaluate_model(model, train, top_k=2)

"""# Workload Management System #"""



import random

class var_3:
    def __init__(self, id, effectiveness_rating, confident_var_3s):
        self.id = id
        self.effectiveness_rating = effectiveness_rating
        self.expertise = confident_var_3s  # List of var_3s var_3 is confident in
        self.var_1s = set()
        self.num_var_1s = 0
        self.is_available = True  # Starts available

    def assign_var_1(self, var_1_id):
        # Flag to check if the var_3 is overworked
        if not self.is_available:
            raise ValueError(f"var_3 {self.id} has reached max capacity")
        else:
            self.var_1s.add(var_1_id)
            self.num_var_1s += 1
            if self.num_var_1s > 10:  # Max capacity for var_3
                self.is_available = False

    def update_availability(self):
        # Dynamically update availability based on workload
        if self.num_var_1s <= 10:
            self.is_available = True
        else:
            self.is_available = False

    def unassign_var_1(self, var_1_id):
        self.var_1s.remove(var_1_id)
        self.num_var_1s -=1
        return

class Cluster:
    def __init__(self, label, product_col):
        self.label = label
        self.available_var_3s = 0
        self.var_3s = []
        self.product_col = product_col
        self.var_3_tracker = {}

    def get_var_3s(self, data):
        var_3_ids = data['agntnum'].unique()
        var_3_effectiveness = data[f'pct_prod_{self.product_col}_cnvrt']  # Effectiveness rating
        var_3_confident_var_3s = data['var_3_product_expertise']  # List of var_3s each var_3 is confident in

        # Function to check confidence based on the product column
        def confident(var_2_id, var_3_confident_var_3s):
            if var_2_id in var_3_confident_var_3s:
                return 1
            else:
                return 0

        # Create var_3s and sort them based on effectiveness and confidence
        self.var_3s = sorted(
            [
                var_3(var_3_id, rating, expertise)
                for var_3_id, rating, expertise in zip(var_3_ids, var_3_effectiveness, var_3_confident_var_3s)
            ],
            key=lambda x: (x.effectiveness_rating, confident(self.product_col, x.expertise)),
            reverse=True
        )

        for var_3 in self.var_3s:
            self.var_3_tracker[var_3.id] = var_3

        self.available_var_3s = len(self.var_3s)

    def assign_next_available_var_3(self, var_1_id):
        for var_3 in self.var_3s:
            if var_3.is_available:
                var_3.assign_var_1(var_1_id)
                return var_3.id
        # No available var_3 in this cluster
        return 0

class WorkloadManager:
    def __init__(self, var_3_df):
        self.clusters = {}
        self.total_var_3s = 0
        self.var_2_to_cluster = {
            0: [5], 2: [7], 4: [4, 3],
            6: [9, 0], 7: [4], 8: [8, 1, 2],
            9: [5]
        }
        # set up var_1 to (var_3, cluster label) pairs
        self.var_1_assignments = {}
        self.unassigned_var_1s = []

        # Reverse mapping from cluster to var_3s (this needs to be a dictionary, not a set)
        self.cluster_to_var_2 = {}
        for var_2_id, cluster_list in self.var_2_to_cluster.items():
            for cluster_id in cluster_list:
                self.cluster_to_var_2[cluster_id] = var_2_id

        # Populate clusters based on var_3_df
        for label in var_3_df['cluster'].unique():
            # Get the relevant var_2(s) for this cluster
            var_2_for_cluster = self.cluster_to_var_2.get(label)
            if var_2_for_cluster is not None:
                cluster = Cluster(label, product_col=var_2_for_cluster)  # Assign the appropriate var_2's product column
                cluster_data = var_3_df[var_3_df['cluster'] == label]
                cluster.get_var_3s(cluster_data)
                self.clusters[label] = cluster

        self.total_var_3s = sum([cluster.available_var_3s for cluster in self.clusters.values()])

    def assign_var_1_to_var_3(self, recommended_var_3s, var_1_id):
        if var_1_id in self.var_1_assignments.keys():
            print(f"var_1 with id {var_1_id} has already been assigned.")
            return

        # Attempt to assign a var_1 to an available var_3 across recommended clusters
        var_2_queue = recommended_var_3s[:]

        while var_2_queue:
            var_2_id = random.choice(var_2_queue)
            recommended_clusters = self.var_2_to_cluster.get(var_2_id, [])

            for cluster_id in recommended_clusters:
                cluster = self.clusters.get(cluster_id, None)
                if cluster:
                    res = cluster.assign_next_available_var_3(var_1_id)
                    if res != 0:
                        # Successfully assigned, record assignment
                        self.var_1_assignments[var_1_id] = [res, cluster.label]
                        print(f"var_1 {var_1_id} assigned to var_3 {res} in cluster {cluster.label}")
                        return

                else:
                    print(f"Cluster {cluster_id} does not exist for var_2 {var_2_id}")

            # If all clusters for the current var_2 are full, remove the var_2 and reattempt with a new one
            var_2_queue.remove(var_2_id)

        # If all var_3s are exhausted and no assignment was made, raise an exception
        print(f"Could not assign var_1 {var_1_id} to any var_3 for the recommended var_3s.")
        self.unassigned_var_1s.append(var_1_id)
        return


    def update_var_3s_availability(self):
        # Update var_3 availability after each assignment
        for cluster in self.clusters.values():
            for var_3 in cluster.var_3s:
                var_3.update_availability()

    def unassign_var_1(self, var_1_id):
        assigned_var_3, cluster_label = self.var_1_assignments[var_1_id][0], self.var_1_assignments[var_1_id][1]
        print(assigned_var_3, cluster_label)
        cluster = self.clusters[cluster_label]
        cluster.var_3_tracker[assigned_var_3].unassign_var_1(var_1_id)
        print(f"var_1 {var_1_id} has been unassigned from {assigned_var_3} in Cluster {cluster_label}")
        return

"""# Full Integration #"""

final_model = tf.keras.models.load_model('XLMicro_v3.keras')

TOP_K=3

def deploy():
  # Step 1: create global workload manager
  workload_manager = WorkloadManager(var_3_df)
  for sample in test:
    # Step 2: get the recommended_var_3s
    var_1_id = sample["var_1_id"].numpy()
    true_var_2 = sample["var_2_id"].numpy()
    var_1_features = sample["var_1_features"].numpy()
    recommended_var_3s, _ = recommend_var_3s(final_model, var_1_id, var_1_features, top_k=TOP_K)
    # Step 3: assign var_1s accordingly
    workload_manager.assign_var_1_to_var_3(recommended_var_3s, var_1_id)

  workload_manager.update_var_3s_availability()
  print(f"Workload Manager with {workload_manager.total_var_3s} var_3s created.")
  return workload_manager

workload_manager = deploy()

# now we can see the features of the workload manager

workload_manager.var_1_assignments

workload_manager.clusters[1].var_3s[0].id
\end{document}
